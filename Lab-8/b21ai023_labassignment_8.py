# -*- coding: utf-8 -*-
"""B21AI023_LabAssignment_8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fnrsrxbte6QoXq5HB9LOYVJ06QJBAuQT

## **Question 1**
"""

!pip install mlxtend
!pip install mlxtend --upgrade --no-deps
from mlxtend.feature_selection import SequentialFeatureSelector as SFS

"""### **Part 1**"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

dataset1 = pd.read_csv("/content/drive/MyDrive/prml/lab-8/train.csv")
dataset1.head()

dataset1.info()

dataset1.drop(['Unnamed: 0'],axis=1,inplace=True)

#Encoding the categorical variables using Label Encoding
from sklearn.preprocessing import LabelEncoder, StandardScaler

cat_cols = ['Gender', 'Customer Type', 'Type of Travel', 'Class','satisfaction']
le = LabelEncoder()
for col in cat_cols:
    dataset1[col] = le.fit_transform(dataset1[col])

dataset1.head()

dataset1.dropna(axis=0,inplace=True)

from sklearn.impute import SimpleImputer

# Replace '-' with NaN
dataset1 = dataset1.replace('-', np.nan)

# Convert all columns to float
dataset1 = dataset1.astype(float)

# Impute missing values with mean
imputer = SimpleImputer()
df = pd.DataFrame(imputer.fit_transform(dataset1), columns=dataset1.columns)

#Splitting the dataset into X and Y variables
X_que1 = dataset1.drop('satisfaction', axis=1)
y_que1 = dataset1['satisfaction']

"""### **Part 2**"""

import json
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

# Split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_que1, y_que1, test_size=0.3, random_state=42)

# Create an object of Decision Tree classifier
dt = DecisionTreeClassifier()
# dt.fit(X_train,y_train)

# Create an object of SFS with Decision Tree classifier object
sfs = SFS(estimator=dt, 
          k_features=10, 
          forward=True, 
          floating=False, 
          scoring='accuracy')

# Train SFS on the dataset
sfs.fit(X_train, y_train)

print('Best Features:')
print(sfs.k_feature_names_)

dt_sfs = DecisionTreeClassifier().fit(X_train[list(sfs.k_feature_names_)],y_train)

print(f"Accuracy on best 10 features selected from sfs is : {dt_sfs.score(X_test[list(sfs.k_feature_names_)],y_test)*100}%")

dt.fit(X_train,y_train)
print(f"Accuracy on using all features is : {dt.score(X_test,y_test)*100}%")

"""### **Part 3**"""

toggle_parameters = [(True,False,'SFS'),(False,False,'SBS'),(True,True,'SFFS'),(False,True,'SBFS')]
models = []
for forw,float_,config in toggle_parameters:
    dtree = DecisionTreeClassifier()
    dtree.fit(X_train,y_train)
    sfs_obj = SFS(dtree,k_features=10,scoring='accuracy',forward=forw,floating=float_,cv=4)
    sfs_obj.fit(X_train.values,y_train.values)
    models.append(sfs_obj)
    print(f"CV score for configuration {config} is: {sfs_obj.k_score_}")

"""### **Part 4**"""

df= pd.DataFrame([])
for m in models:
    temp_dict = m.get_metric_dict()[10]
    temp_dict['cv_scores'] = list(temp_dict['cv_scores'])
    temp_dict['feature_idx'] = list(temp_dict['feature_idx'])
    temp_dict['feature_names'] = list(temp_dict['feature_names'])
    df=pd.concat([df,pd.json_normalize(temp_dict)],axis=0)
df.index = ['SFS','SBS','SFFS','SBFS']
df

from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs

config_name = ['Sequential Forward Selection','Sequential Backward Selection','Sequential Forward Floating Selection','Sequential Backward Floating Selection']
i=0
for mod in models:
    fig1 = plot_sfs(mod.get_metric_dict(),kind='std_dev')
    plt.title(config_name[i])
    i+=1
    plt.grid()
    plt.show()

"""### **Part 5 and Part 6**"""

# Define the selection criteria
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.feature_selection import SelectKBest, chi2
from scipy.spatial.distance import cdist


def accuracy_measure_dtc(X_train, y_train, X_test, y_test):
    clf1 = DecisionTreeClassifier()
    clf1.fit(X_train, y_train)
    y_pred1 = clf1.predict(X_test)
    score1 = accuracy_score(y_test, y_pred1)
    return score1

def accuracy_measure_svm(X_train, y_train, X_test, y_test):
    clf = SVC()
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    score = accuracy_score(y_test, y_pred)
    return score

def information_measure(X_train, y_train, X_test, y_test):
    selector = SelectKBest(chi2, k='all')
    selector.fit(X_train, y_train)
    scores = selector.scores_
    # print(scores)
    return scores

def feature_selection(X_train, y_train, X_test, y_test, selection_criteria):
    # Initialize an empty list to keep track of selected features
    selected_features = []
    
    # Loop over all features in the dataset
    for feature in X_train.columns:
        # Check if the feature has already been selected
        if feature in selected_features:
            continue
        
        # Add the current feature to the list of selected features
        selected_features.append(feature)
        
        # Evaluate the current feature set using the selection criteria
        current_score = selection_criteria(X_train[selected_features], y_train, X_test[selected_features], y_test)
        # print(current_score)
        # Loop over all remaining features in the dataset
        for remaining_feature in X_train.columns:
            # Check if the remaining feature has already been selected
            if remaining_feature in selected_features:
                continue
            
            # Add the remaining feature to the list of selected features
            current_features = selected_features + [remaining_feature]
            
            # Evaluate the current feature set using the selection criteria
            new_score = selection_criteria(X_train[current_features], y_train, X_test[current_features], y_test)
            # print(new_score)
            # If the new score is better than the current score, update the selected features
            if new_score.mean() > current_score.mean():
                current_score = new_score.mean()
                selected_features = current_features
    
    # Return the final list of selected features
    return selected_features

selected_features_accuracy = feature_selection(X_train, y_train, X_test, y_test, accuracy_measure_dtc)
selected_features_information = feature_selection(X_train, y_train, X_test, y_test, information_measure)

print('Selected features (accuracy measure):', selected_features_accuracy)
print('Selected features (information measure):', selected_features_information)

"""### **Part 7**"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

rf_acc_dt = RandomForestClassifier()
rf_acc_dt.fit(X_train[selected_features_accuracy], y_train)
cv_acc_dt = cross_val_score(rf_acc_dt, X_train[selected_features_accuracy], y_train, cv=5)
print("Accuracy using Decision Tree and Random Forest:", np.mean(cv_acc_dt))

rf_info_gain = RandomForestClassifier()
rf_info_gain.fit(X_train[selected_features_information], y_train)
cv_info_gain = cross_val_score(rf_info_gain, X_train[selected_features_information], y_train, cv=5)
print("Accuracy using Information Gain and Random Forest:", np.mean(cv_info_gain))

"""## **Question 2**

### **Part 1**
"""

#Generating a dataset of 1000 points from a zero-centred Gaussian distribution with a covariance matrix

import numpy as np
import plotly.graph_objs as go

# Set the mean and covariance matrix
mean = [0, 0, 0]
cov = [[0.6006771,0.14889879,0.244939], [0.14889879,0.58982531,0.24154981], [0.244939,0.24154981,0.48778655]]

# Generate the dataset
dataset2 = np.random.multivariate_normal(mean, cov, 1000)

# Compute the dot product of each data point with the vector v
v = [[1/np.sqrt(6)], [1/np.sqrt(6)], [-2/np.sqrt(6)]]
dot_product = np.dot(dataset2, v)

# Assign labels based on the dot product
labels = np.where(dot_product > 0, 0, 1)

# Create a 3D scatter plot using Plotly
fig = go.Figure(data=[go.Scatter3d(x=dataset2[:,0], y=dataset2[:,1], z=dataset2[:,2], mode='markers', marker=dict(size=4,color=labels))])
fig.show()

"""### **Part 2**"""

from sklearn.decomposition import PCA

# Applying PCA with n_components=3
pca = PCA(n_components=3)
# pca.fit(dataset2)
# transformed_data = pca.transform(dataset2)

transformed_data = pca.fit_transform(dataset2)

# Create a 3D scatter plot using Plotly
fig = go.Figure(data=[go.Scatter3d(x=transformed_data[:,0], y=transformed_data[:,1], z=transformed_data[:,2], mode='markers',marker=dict(size=4,color=labels))])
fig.show()

"""### **Part 3**"""

from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt

# Perform complete feature selection on the transformed data with a number of features in the subset equal to 2
n_features = 2
combs = []
for i in range(3):
    for j in range(i+1, 3):
        combs.append((i,j))

# accuracies = []

# Fit a decision tree for every subset-set of features of size 2 and plot their decision boundaries superimposed with the data
plt.figure(figsize=(16, 12))
for i, (f1, f2) in enumerate(combs):

    # Select the subset of features
    X = transformed_data[:, [f1, f2]]

    # Fit a decision tree
    clf = DecisionTreeClassifier()
    clf.fit(X, labels)

    # # Calculate the accuracy of the decision tree
    # y_pred = clf.predict(X)
    # acc = accuracy_score(labels, y_pred)
    # accuracies.append(acc)
    # print(f"Features {f1+1} and {f2+1}: Accuracy = {acc:.2f}")

    # Create a meshgrid of points to plot the decision boundary
    xx, yy = np.meshgrid(np.linspace(X[:, 0].min() - 0.5, X[:, 0].max() + 0.5, 100),
                         np.linspace(X[:, 1].min() - 0.5, X[:, 1].max() + 0.5, 100))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    # Plot the decision boundary and the data
    plt.subplot(2, 3, i+1)
    plt.title(f"Features {f1+1} and {f2+1}")
    plt.contourf(xx, yy, Z, alpha=0.2, cmap='coolwarm')
    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='coolwarm', edgecolors='k')
plt.show()

"""### **Part 4**"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Select the subset of features obtained by applying PCA with n_components=2 and fit a decision tree
X_pca = transformed_data[:, :2]

# split the data
X_train, X_test, y_train, y_test = train_test_split(X_pca, labels, test_size=0.2, random_state=42)

clf_pca = DecisionTreeClassifier()
clf_pca.fit(X_train, y_train)

# Calculate the accuracy of the decision tree
y_pred_pca = clf_pca.predict(X_test)
acc_pca = accuracy_score(y_test, y_pred_pca)
print(f"PCA Features: Accuracy = {acc_pca:.4f}")

# Fit a decision tree for every subset-set of features of size 2 and calculate their accuracies
accuracies = []
for i, (f1, f2) in enumerate(combs):
    # Select the subset of features
    X = transformed_data[:, [f1, f2]]

    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)

    # Fit a decision tree
    clf = DecisionTreeClassifier()
    clf.fit(X_train, y_train)

    # Calculate the accuracy of the decision tree
    y_pred = clf.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    accuracies.append(acc)
    print(f"Features {f1+1} and {f2+1}: Accuracy = {acc:.4f}")

# Plot the accuracies
plt.figure(figsize=(10, 5))
plt.bar(range(len(combs)), accuracies, tick_label=[f"Features {f1+1} and {f2+1}" for f1, f2 in combs])
plt.axhline(y=acc_pca, color='r', linestyle='-', label="PCA Features")
plt.xticks(rotation=90)
plt.xlabel("Feature Subset")
plt.ylabel("Accuracy")
plt.legend()
plt.show()