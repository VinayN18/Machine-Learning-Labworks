# -*- coding: utf-8 -*-
"""B21AI023_Lab_Assignment_11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UZDm3N3n6Kb19vrVLue1EihUwJYdvMpI
"""

from google.colab import drive
drive.mount('/content/drive')

"""## **Question 1**

## **Part 1**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv("/content/drive/MyDrive/prml/lab-11/data_banknote_authentication.csv", header=None,names=['variance', 'skewness', 'curtosis', 'entropy', 'class'])
data

data.info()

data.describe()

#checking the empty values
data.isnull()
sns.heatmap(data.isnull(),yticklabels=False,cbar=False)
'''Cream Lines in the graph indicates the empty values'''

"""Since there are no cream lines the dataset has no missing values"""

from sklearn.preprocessing import StandardScaler

# Split the dataset into features and target
X = data.drop('class', axis=1)
y = data['class']

# Normalize the data using StandardScaler
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split the dataset into training, testing, and validation sets with 70:20:10 ratio
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.33, random_state=42)

# Print the shape of each set
print("Training set shape:", X_train.shape, y_train.shape)
print("Validation set shape:", X_val.shape, y_val.shape)
print("Testing set shape:", X_test.shape, y_test.shape)

"""### **Part 2**"""

from sklearn.metrics import accuracy_score
from sklearn.svm import SVC

# Define a list of values for C
C_values = [0.1, 1, 10, 100, 1000]

# Train the SVM classifier for each value of C and evaluate its performance
for C in C_values:
    # Create an instance of SVM classifier with a linear kernel
    svm = SVC(kernel='linear', C=C)

    # Train the classifier on the training data
    svm.fit(X_train, y_train)

    # Make predictions on the testing data
    y_pred = svm.predict(X_test)

    # Compute the classification accuracy of the classifier
    accuracy = accuracy_score(y_test, y_pred)

    print(f'C = {C}, Accuracy = {accuracy}')

data.corr()
corrplot = sns.heatmap(data.corr(), cmap="YlGnBu", annot=True)

"""Clearly we can see Curtosis and Entropy high correlation with the class column"""

# Compute the correlation matrix
corr_matrix = data.corr()

# Select the two features that have the highest correlation with the target
features = np.argsort(np.abs(corr_matrix['class']))[-5:-3]
print('Selected features:', corr_matrix.index[features])

# Extract the selected features from the data
X_features = X[:, features]

# Plot the decision boundaries for each value of C
for i, C in enumerate(C_values):
    # Create an instance of SVM classifier with a linear kernel
    svm = SVC(C=C)
    y = y
    # Train the classifier on the data
    svm.fit(X_features, y)

    # Define the mesh grid for plotting
    x_min, x_max = X_features[:, 0].min() - 1, X_features[:, 0].max() + 1
    y_min, y_max = X_features[:, 1].min() - 1, X_features[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                         np.arange(y_min, y_max, 0.02))
    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    # Plot the decision boundaries
    plt.figure(figsize=(8, 6))
    plt.title(f'C = {C}')
    plt.contourf(xx, yy, Z, cmap='coolwarm', alpha=0.4)

    # Plot the training points
    plt.scatter(X_features[:, 0], X_features[:, 1], c=y, cmap='coolwarm', s=20, edgecolors='k')
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.show()

"""### **Part 3**"""

# Define a list of kernels to use
kernels = ['linear', 'poly', 'rbf', 'sigmoid']

# Train the SVM models using different kernels
for kernel in kernels:
    svm = SVC(kernel=kernel)
    svm.fit(X_features, y.ravel())

    X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size=0.3, random_state=42)
    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.33, random_state=42)

    y_pred = svm.predict(X_test)
    accuracy = accuracy_score(y_test,y_pred)
    print("Accuracy is  : ", accuracy * 100, "when kernel is ",kernel)
    
    # Plot the decision boundary
    plt.figure(figsize=(8, 6))
    plt.title(f'{kernel.capitalize()} kernel')
    x_min, x_max = X_features[:, 0].min() - 1, X_features[:, 0].max() + 1
    y_min, y_max = X_features[:, 1].min() - 1, X_features[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                         np.arange(y_min, y_max, 0.02))
    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, cmap='coolwarm', alpha=0.4)
    plt.scatter(X_features[:, 0], X_features[:, 1], c=y, cmap='coolwarm', s=20, edgecolors='k')
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.show()