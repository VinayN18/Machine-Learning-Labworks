# -*- coding: utf-8 -*-
"""B21AI023_Lab4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CK2w5rM1WXzqHgKWAKe6aurr8m2_P9_R

## **Question 1**
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

dataset1 = pd.read_csv("/content/drive/MyDrive/prml/lab-4/Iris.csv")
dataset1.head()

dataset1.info()

dataset1.describe()

"""### **Preprocessing the data, Visualising and Splitting**"""

#Checking the empty values in columns
dataset1.isnull()
sns.heatmap(dataset1.isnull(),yticklabels=False,cbar=False)
''' Since all the values are given there are no empty values'''

#Dropping the Id column since it is not necessary
dataset1.drop('Id',axis=1,inplace=True)
dataset1.head()

#Since there are no empty values and all the values are integers, there is no need to perform any particular data preprocessing
print(dataset1['Species'].value_counts()/len(dataset1))
fig,ax= plt.subplots()
dataset1['Species'].value_counts().plot(ax=ax, kind='pie')
plt.show()

#Bivariate scatterplots and univariate histograms between SepalWidth and SepalLength
sns.jointplot(x="SepalLengthCm", y="SepalWidthCm", data=dataset1, height=5)
plt.show()

#Bivariate scatterplots and univariate histograms between PetalWidth and PetalLength
sns.jointplot(x="PetalLengthCm", y="PetalWidthCm", data=dataset1, height=5)
plt.show()

sns.pairplot(dataset1, hue="Species", height=3)
plt.show()

dataset1 = dataset1.replace({'Species': {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica':2}})
dataset1.head()

"""### **Splitting Dataset**"""

from sklearn.model_selection import train_test_split
# Split the data into a training set and a test set
X = dataset1.drop('Species',axis=1)
y = dataset1['Species']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""### **Gaussian Bayes Classifier from Scratch**"""

class GaussianBayesClassifier():
  
  def __init__(self):
    # self.case = case
    self.feature=None
    self.mean_p=None
    self.mean_q=None
    self.mean_n=None
    self.std_p=None
    self.std_q=None
    self.std_n=None
    self.prior_p=None
    self.prior_q=None
    self.prior_n=None

  def fit(self,x,y):
      p = x[y==1]
      n = x[y==0]
      q = x[y==2]
      self.prior_p = np.sum(p)/dataset1.shape[0]
      self.prior_q = np.sum(q)/dataset1.shape[0]
      self.prior_n = 1- self.prior_p - self.prior_q

      self.feature = []
      self.mean_p = []
      self.std_p = []
      self.mean_n = []
      self.std_n = []
      self.mean_q = []
      self.std_q = []

      for f in x.columns:
        self.feature.append(f)
        self.mean_p.append(np.mean(p[f]))
        self.mean_n.append(np.mean(n[f]))
        self.mean_q.append(np.mean(q[f]))
        self.std_p.append(np.std(p[f]))
        self.std_n.append(np.std(n[f]))
        self.std_q.append(np.std(q[f]))

  def gauss(self,std,mean,x):
      val=((x-mean)/std)**2
      k=np.sqrt(2*np.pi)*std
      sol=np.exp(-0.5*val) 
      return sol/k 

  def predict(self,x):
         prediction=[]
         prob=[]
         residue=[]

         num, feats = x.shape
         for i in range(num):
           like_p=1
           like_n=1
           like_q=1
           row= x.iloc[i,:]
           for fi in range(feats):
             like_p*= self.gauss(self.std_p[fi], self.mean_p[fi], row[fi])
             like_n*= self.gauss(self.std_n[fi], self.mean_n[fi], row[fi])
             like_q*= self.gauss(self.std_q[fi], self.mean_q[fi], row[fi])

          #  prediction.append(1 if like_p>like_n and like_p>like_q else 0)
           if (like_p>=like_q and like_p>=like_n):
              prediction.append(1)
              prob.append(like_p)
              residue.append(like_p)
           elif (like_q>like_p and like_q>like_n):
             prediction.append(2)
             prob.append(like_q)
             residue.append(like_q)
           else:
             prediction.append(0)
             prob.append(like_n)
             residue.append(like_n)
          #  prob.append(like_p if like_p>like_n else like_n)
          #  residue.append(like_p if like_p<=like_n else like_n)

         prob = np.array(prob) / (np.array(prob)+np.array(residue))
         sol = [np.array(prediction),prob]
         return sol
  def train(self,y_pred,y_train):
    accuracy = np.sum(y_pred == y_train)/y_pred.shape[0]
    return accuracy

# Initialize the classifier with type 's2.I'
gb = GaussianBayesClassifier()

# Train the model on training data
gb.fit(X_train, y_train)

# Predict class for a single data point
y_pred_gb = gb.predict(X_train)[0]
y_pred_prob = gb.predict(X_train)[0]

#Test the model 
accuracy = gb.train(y_pred_gb,y_train)
print('Accuracy of model on Train set: ', accuracy)
print(f'Mean Confidence for all predictions: ', np.mean(y_pred_prob))

"""### **Performing 5 fold Cross-Validation**"""

from sklearn.model_selection import KFold,cross_val_score

kfold = KFold(n_splits=5, random_state=None, shuffle=True)

#Initializing a list to store the accuracy results
accuracy_results = []

#Copying the data in an array
dataset_copy = np.array(X)

#Looping through each fold
for train_index, test_index in kfold.split(dataset_copy):
  x_train, x_test = dataset_copy[train_index], dataset_copy[test_index]
  Y_train, Y_test = y[train_index], y[test_index]

  #Training the Gaussian Naive Bayes Classifier
  model = GaussianBayesClassifier()
  # model.fit(x_train, Y_train)

  #Making predictions on test data
  Y_predictions = model.predict(x_test)
  p = model.predict(x_test)[1]
  q = model.predict(x_test)[0]
  #Calculating accuracy score
  acc = model.test(Y_predictions,Y_test)

  #Appending the accuracy score to the results list
  accuracy_results.append(acc)
    
print("Accuracy results for 5 fold Cross-validation\n",accuracy_results)
# Calculating the average accuracy across all folds
average_accuracy = np.mean(accuracy_results)
print("\nAverage accuracy:", average_accuracy,'\n\n')

""" # **Question 2**"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

##Covariance matrix and mean are given in the question
mean = np.array([0,0])
covariance = np.array([[3/2,1/2],[1/2,3/2]])
print("Mean is: ",mean)
print("Covariance is: \n",covariance)
d = 2 #since the covariance matrix has order 2*2 (d*d), we know that d=2
n=1000 #let us take 1000 samples

"""### **Getting sample random points from the multivariate normal distribution**"""

X = np.random.multivariate_normal(mean=mean, cov=covariance, size=n)
x_trans = np.transpose(X)

#Plotting the data points
plt.scatter(X[:,0],X[:,1])

# Plotting density function.
sns.jointplot(x=x_trans[0], y=x_trans[1], kind="kde", space=0);

"""### **a) Covariance matrix of the sample** """

#Mean of each variable in the sample
mean = np.mean(X, axis=0)
print("Mean of each variable in the sample: ",mean)

#Subtract the mean from each data in the sample to get the centred data
centred_data = X - mean
print("\nCentred sample: \n", centred_data)

#Calculating covariance matrix between each pair of variables
cov_matrix = np.dot(centred_data.T,centred_data) / (X.shape[0]-1)
print("\nCovariance Matrix of the sample: \n",cov_matrix)

from numpy.linalg import eig
eigen_value, eigen_vector = eig(cov_matrix)
print("Eigen Values are: ",eigen_value)
print("\nEigen Vector: \n",eigen_vector)

#Plotting the eigenvalues and eigen vectors superimposed on the dataset
#plotting the sample data
plt.scatter(X[:,0],X[:,1])

#Plotting the eigenvectors
for ev in eigen_vector.T:
  plt.quiver(0,0,ev[0],ev[1],color='red',scale=eigen_value[0])

#Plotting the eigenvalues
for i,e_value in enumerate(eigen_value):
  plt.text(X[i,0], X[i,1], str(e_value), color='black', fontsize=10)
plt.xlim(-5,5)
plt.ylim(-5,5)
plt.show()

"""### **Perform the transformation and finding the covariance matrix** """

from scipy.linalg import fractional_matrix_power

# Calculate the square root of the covariance matrix
sqrt_cov = fractional_matrix_power(cov_matrix, 0.5)

# Perform the transformation
y = np.dot(X, sqrt_cov)

# Calculate the covariance matrix of the transferred data
mean_y = np.mean(y, axis=0)
print("Mean of each variable in the transformed sample: ",mean_y)

centred_data_y = y - mean_y
print("\nCentred sample: \n", centred_data_y)

cov_matrix_y = np.dot(centred_data_y.T,centred_data_y) / (y.shape[0]-1)
print("\nCovariance Matrix of the transformed sample: \n",cov_matrix_y)

eigen_value, eigen_vector = eig(cov_matrix)
print("\nEigen Values of covariance matrix of Y are: ",eigen_value)
print("\nEigen Vector of covariance matrix of Y: \n",eigen_vector)

"""### The purpose of transformation is it can be used to rotate or rescale the data in order to allign the principal components of the data with the axes of the plot. This can simplify the visualisation of the data by reducing the dimensionality of the data and highlighting the most important features or patterns in data. It also helps in normaizing the data or to standardize the variables.
### Covariance matrix obtained after transformation has higher values than the previous one.

### **Part C**
"""

theta = np.linspace(0, 2 * np.pi, 10)
r = np.sqrt(25)
x_dp = r * np.cos(theta)
y_dp = r * np.sin(theta)
P = np.column_stack((x_dp, y_dp))

sns.scatterplot(x=x_dp, y=y_dp, hue=theta, palette='viridis')
plt.scatter(X[:,0],X[:,1])

#Plotting the Eucledian distance of each point from mean

mu = np.mean(P, axis=0)
d = np.sqrt(np.sum((P - mu)**2, axis=1))
plt.bar(np.arange(len(d)), d)
plt.xlabel("Sample")
plt.ylabel("Euclidean Distance from Mean")
plt.show()

"""### **Part d**"""

#Finding covariance matrix
#We calculated the mean of the data points
#Subtracting the mean from each data point
p_centred = P - mu

covariance_matrix_p = np.cov(p_centred, rowvar=False)
print("Covariance matrix of data points\n",covariance_matrix_p)

##Transformation of data points and calculating covariance matrix

from scipy.linalg import fractional_matrix_power

# Calculate the square root of the covariance matrix
sqrt_cov_p = fractional_matrix_power(covariance_matrix_p, 0.5)

# Perform the transformation
q_p = np.dot(P, sqrt_cov_p)

# Calculate the covariance matrix of the transferred data
mean_q_p = np.mean(q_p, axis=0)
print("Mean of each variable in the transformed sample: ",mean_q_p)

centred_data_q_p = q_p - mean_q_p
print("\nCentred Sample: \n", centred_data_q_p)

cov_matrix_q_p = np.dot(centred_data_q_p.T,centred_data_q_p) / (q_p.shape[0]-1)
print("\nCovariance Matrix of the transformed dataponts: \n",cov_matrix_q_p)

#Calculating Eucledian distance of transformed data points
d_q = np.sqrt(np.sum((q_p - mu)**2, axis=1))

plt.bar(np.arange(len(d_q)), d_q)
plt.xlabel("Sample")
plt.ylabel("Euclidean Distance of transferred datapoints from Mean")
plt.show()

#Plotting points in Q along with datapoints in Y
sns.scatterplot(x=q_p[:,0], y=q_p[:,1], hue=theta, palette='viridis')
plt.scatter(y[:,0],y[:,1])

"""The graph of Eucledian distance of data points from the mean before was first increasing and decreasing but the grpah of eucledian distance of transferred data points from the mean was like first decreasing, and then increasing and then decreasing and increasing """