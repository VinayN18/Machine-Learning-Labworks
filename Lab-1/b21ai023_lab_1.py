# -*- coding: utf-8 -*-
"""B21AI023_Lab_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o_ZA9js_t6lKJV3Vg0HvLmq7X_7k7FNi

**Problem 1**
"""

# a) Convert file data to list
import pandas as pd
data=pd.read_csv('/content/drive/MyDrive/prml/lab_1/data.csv')
lis=[]
values = data.keys()
for i in values:
  for j in data[i]:
    lis.append(j)
print("CSV to List: \n",lis)

# b) Convert User Input to a Number
x = int(input("Enter any Number: "))
print('Number: ',x)

# c) Convert String to Datetime in python
from datetime import datetime
dateTime = "Jun 18 2003 6:30PM"
variable = datetime.strptime(dateTime, '%b %d %Y %I:%M%p')
print('Date: ',variable.date())
print('Time: ', variable.time())

# d) How to call external commands in python
import os
os.system("dir *.md")

# e) How to count the occurences of a list item
lis = [1,1,1,2,2,3,4,5,6,6,6,6,10,10,15]
print('Count of 1: ',lis.count(1))
print('Count of 2: ',lis.count(2))
print('Count of 3: ',lis.count(3))
print('Count of 4: ',lis.count(4))
print('Count of 5: ',lis.count(5))
print('Count of 6: ',lis.count(6))
print('Count of 10: ',lis.count(10))
print('Count of 15: ',lis.count(15))

# f) How to flatten lists in python
lis = [ [3,4,5], ['a','b','c','d'], [1.50,25,4.50]]
lis = [i for inner_list in lis for i in inner_list]
print(lis)

# g) How to merge dictionaries in python
x = {'Name':'Vinay', 'Place':'Vizag', 'Branch':'AI&DS'}
y = {'Hobies':'Badminton', 'Height in cm': 184}
z=x.copy()
z.update(y)
print(z)

# h) Remove duplicate items from a list in python
lis = [1,1,1,2,2,3,4,5,6,6,6,6,10,10,15]
lis= list(set(lis))
print(lis)

# i) Python script to check whether a given key already exists in a dictionary.
x={'Name': 'Vinay', 'Place': 'Vizag', 'Branch': 'AI&DS', 'Hobies': 'Badminton', 'Height in cm': 184}
keys=['college','Name','Branch','Place','Friends']
for i in keys:
  if(i in x.keys()): 
    print(i,True)
  else:
    print(i,False)

""" **Problem 2**"""

import numpy as np

#creating two matrices using numpy
x=np.arange(1,10)
x.resize((3,3))
print('First Matrix: \n',x,'\n')
y=np.arange(5,15)
y.resize((3,3))
print('Second Matrix: \n',y,'\n\n')

# a) Display first row of first matrix
print("First row of first matrix: ", x[0],'\n')

# b) Display second column of second matrix
print("Second column of second matrix: ",y[:,1],'\n')

# c) Matrix multiplication 
print('x.y \n',x.dot(y),'\n')

# d) Element-wise multiplication
print('Element-wise multiplication: \n', x*y,'\n')

# e) Dot product between each column of first and second matrix
#first we take the transpose of both the matrices
x_transpose = x.T
y_transpose = y.T
print('Dot product of columns')
for i in x_transpose:
  for j in y_transpose:
    print('Dot product between',i,"and",j,'is: ', i.dot(j)) #multiplying the columns

"""**Problem 3**"""

import pandas as pd

#Input the dataset
data = pd.read_csv('/content/drive/MyDrive/prml/lab_1/Cars93.csv')
data.info()
data.head()

# i) Assign a type of the following features
'''
Ordinal Scale: Type, AirBags
Nominal Scale: Model
Ratio Scale: Max.Price
Interval Scale: MPG.city, MPG.highway
'''

# ii) Function to handle the missing values in dataset
#The rear seat room and lugage room are missing for some instances, they need to be taken care of
#About 11 instances have missing lugage room, dropping the models of the missing dataset is not feasible. So we are going to use mean.
data.dropna(inplace=True)
data.head()

# iii) Function to reduce noise (any error in the feature) in individual attributes
#By carefully observing the dataset we can get to know that there is some noise in model names

col = ['Model'] #columns with noise in them
for i in col:
  value = data[i] #values of models having noise
  for j in data.index.values:
    if value[j].isdigit():   #checking for integer models
      data.drop(j, inplace=True) #if integer model then dropping the model

data.reset_index(inplace=True)
data.drop('index', inplace=True, axis=1)
data.head()

# v) Function to normalize the features either individually or jointly.
from sklearn.preprocessing import StandardScaler
s= StandardScaler()
num_features=['Min.Price','Price'	,'Max.Price'	,'MPG.city'	,'MPG.highway' ,'Cylinders'	,'EngineSize'	,'Horsepower'	,'RPM'	,'Rev.per.mile' ,'Fuel.tank.capacity'	,'Passengers',	'Length'	,'Wheelbase'	,'Width'	,'Turn.circle'	,'Rear.seat.room'	,'Luggage.room'	,'Weight']
s.fit(data[num_features])

# vi) Function to create a random split of the data into train, validation and test sets
from sklearn.model_selection import train_test_split
train_set2, test_set = train_test_split(data, test_size=0.1, random_state=42)
train_set, val_set= train_test_split(data, test_size=0.222222, random_state=42)

"""**Problem 4**"""

import matplotlib.pyplot as plt

# a) y = 5x + 4 where x ranges from [-10, 10]
x=np.arange(-10,11)
y = 5*x + 4
plt.plot(x,y)
plt.title('y=5*x+4')
plt.show()

# b) y = ln(x) where x > 10 and x < 100
x=np.linspace(10,100,1000)
y=np.log(x)
plt.plot(x,y)
plt.title('y=ln(x)')
plt.show()

# c) y = x^2 where x ranges from [-10, 10]
x=np.arange(-10,11)
y = x**2
plt.plot(x,y)
plt.title('y=x^2')
plt.show()

"""**Problem 5**"""

## Copied from the collab file which is shared to us
#Import the necessary python libraries and components
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split as tts
from sklearn.linear_model import LogisticRegression as LR
from sklearn.tree import DecisionTreeClassifier as DTC
from sklearn.metrics import confusion_matrix as cm
from sklearn.metrics import precision_score as ps
from sklearn.metrics import recall_score as rs
from sklearn.metrics import f1_score as f1s
from sklearn.metrics import accuracy_score as acc

#To Disable Convergence Warnings (For Custom Training)
from warnings import simplefilter
from sklearn.exceptions import ConvergenceWarning
simplefilter("ignore", category=ConvergenceWarning)

#Input the dataset
data=pd.read_csv('/content/drive/MyDrive/prml/lab_1/data.csv')

#Convert the string labels into easily-interpretable numerics
condition_M = data.diagnosis == "M"
condition_B = data.diagnosis == "B"

data.loc[condition_M,"diagnosis"]=0
data.loc[condition_B,"diagnosis"]=1

#Converting dataframe into numpy arrays (features and labels)
Y = data.diagnosis.to_numpy().astype('int')                    # Labels

X_data = data.drop(columns=["id","diagnosis","Unnamed: 32"])
X = X_data.to_numpy()                                          # Input Features

#Splitting the dataset into train and test portions
user_prompt = 0.3
user_enable = False

x_train,x_test,y_train,y_test = tts(X,Y,test_size=user_prompt,shuffle=user_enable)

#Model training and predicting
logistic_model = LR()
logistic_model.fit(x_train,y_train)
logistic_pred = logistic_model.predict(x_test)

decision_model = DTC()
decision_model.fit(x_train,y_train)
decision_pred = decision_model.predict(x_test)

#Evaluation Metrics (Inbuilt v/s Scratch)
##Confusion Matrix (Inbuilt)
inbuilt_matrix_logistic = cm(y_test,logistic_pred)
inbuilt_matrix_decision = cm(y_test,decision_pred)

print("Confusion Matrix for Logistic Regression-based Predictions =>")
print(inbuilt_matrix_logistic)
print("Confusion Matrix for Decision Tree-based Predictions =>")
print(inbuilt_matrix_decision)

#User-defined Confusion Matrix
def confusion_matrix(y,prediction):
  conf_mat = np.array([[0,0],[0,0]])
  # conf_mat = [[0,0],[0,0]]
  for i in range(len(y)):
    if (y[i]==0 and prediction[i]==0): #True Negative (TN) -> when false class is correctly predicted as false
      conf_mat[0][0]+=1
    if (y[i]==0 and prediction[i]==1): #False Positive (FP) -> when false class is wrongly predicted as true
      conf_mat[0][1]+=1
    if (y[i]==1 and prediction[i]==0): #False Negative (FN) -> when true class is wrongly predicted as flase
      conf_mat[1][0]+=1
    if (y[i]==1 and prediction[i]==1): #True Positive (TP) -> when true class is correctly predicted as true
      conf_mat[1][1]+=1
  return conf_mat

logistic_confmat=confusion_matrix(y_test, logistic_pred)
print("Confusion Matrix for Logistic Regression-based Predictions =>")
print(logistic_confmat)

decisiontree_confmat= confusion_matrix(y_test, decision_pred)
print("Confusion Matrix for Decision Tree-based Predictions =>")
print(decisiontree_confmat)

##Average Accuracy (Inbuilt)
inbuilt_acc_logistic = acc(y_test,logistic_pred)
inbuilt_acc_decision = acc(y_test,decision_pred)

print("Accuracy for Logistic Regression-based Predictions =>",str(inbuilt_acc_logistic*100)+"%")
print("Accuracy for Decision Tree-based Predictions =>",str(inbuilt_acc_decision*100)+"%")
print('\n')

##User-defined Average Accuracy
def avg_accuracy(mat):

  #Average Accuracy = (TN+TP)/(TN+FP+FN+TP)

  avg = (mat[0,0] + mat[1,1]) / (mat[0,0] + mat[0,1] + mat[1,0] + mat[1,1])
  print(100*avg)

print("Average Accuracy for Logistic Regression-based Predictions: ")
avg_accuracy(logistic_confmat)
print('Average Accuracy for Decision Tree-based Predictions: ')
avg_accuracy(decisiontree_confmat)

##Precision (Inbuilt)
inbuilt_ps_logistic = ps(y_test,logistic_pred)
inbuilt_ps_decision = ps(y_test,decision_pred)

print("Precision for Logistic Regression-based Predictions =>",str(inbuilt_ps_logistic*100)+"%")
print("Precision for Decision Tree-based Predictions =>",str(inbuilt_ps_decision*100)+"%")
print('\n')

#User-defined Precision
def precision(mat):

  #Precision = TP/(TP+FP)

  avg = mat[1,1]/(mat[1,1] + mat[0,1])
  print(100*avg)

print('Precision for Logistic Regression-based Predictions')
precision(logistic_confmat)
print('Precision for Decision Tree-based Predictions')
precision(decisiontree_confmat)

##Recall (Inbuilt)
inbuilt_rs_logistic = rs(y_test,logistic_pred)
inbuilt_rs_decision = rs(y_test,decision_pred)

print("Recall for Logistic Regression-based Predictions =>",str(inbuilt_rs_logistic*100)+"%")
print("Recall for Decision Tree-based Predictions =>",str(inbuilt_rs_decision*100)+"%")
print('\n')

#User-defined Recall
def recall(mat):

  #Recall = TP/(TP+FN)

  avg = mat[1,1]/(mat[1,1] + mat[1,0])
  print(avg*100)

print('Recall for Logistic Regression-based Predictions')
recall(logistic_confmat)
print('Recall for Decision Tree-based Predictions')
recall(decisiontree_confmat)

##F-1 Score (Inbuilt)
inbuilt_f1s_logistic = f1s(y_test,logistic_pred)
inbuilt_f1s_decision = f1s(y_test,decision_pred)

print("F1-Score for Logistic Regression-based Predictions =>",str(inbuilt_f1s_logistic*100)+"%")
print("F1-Score for Decision Tree-based Predictions =>",str(inbuilt_f1s_decision*100)+"%")
print('\n')

#User-defined F-1 Score
def f1_score(mat):

  #F-1 Score = Harmonic Mean of Precision and Recall

  pre=mat[1,1]/( mat[1,1]+ mat[0,1])
  re=mat[1,1]/(mat[1,1]+ mat[1,0])
  print( 2*100*pre*re/(pre + re))

print('F1-score for Logistic Regression-based Predictions')
f1_score(logistic_confmat)
print('F1-score for Decision Tree-based Predictions')
f1_score(decisiontree_confmat)

##Class-wise Accuracy
def class_accuracy(mat):

  #Class-wise Accuracy = (TN/(TN+FP)+ TP/(TP+FN))/2
  
  avg1= mat[0,0]/( mat[0,0] + mat[0,1])  ## TN/(TN+FP): Accuracy of Negative class
  avg2= mat[1,1]/(mat[1,1]+ mat[1,0])    ## TP/(TP+FN): Accuracy of Positive class
  print( 100*(avg1+avg2)/2)

print('Class-wise Accuracy for Logistic Regression-based Predictions: ')
class_accuracy(logistic_confmat) 
print('class-wise Accuracy for Decision Tree-based Predictions: ')
class_accuracy(decisiontree_confmat)

##Sensitivity
def sensitivity(mat):

  #Sensitivity: Proportion of actual positives which are correctly identified same as recall
  #Sensitivity = TP/(TP+FN)
  
  print(100*mat[1,1]/(mat[1,1]+ mat[1,0]))

print('Sensitivity for Logistic Regression-based Predictions')
sensitivity(logistic_confmat)
print('Sensitivity for Decision Tree-based Predictions')
sensitivity(decisiontree_confmat)

##Specificity
def specificity(mat):

  #Specificity: Proportion of actual negatives which are correctly identified
  #Specificity = TN/(TN+FP)

  print(100*mat[0,0]/(mat[0,0]+ mat[0,1]))

print('Specificity for Logistic Regression-based Predictions')
specificity(logistic_confmat)
print('Specificity for Decision Tree-based Predictions')
specificity(decisiontree_confmat)